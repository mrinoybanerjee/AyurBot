{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AyurBot: Using RAG Based LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "MONGODB_CONNECTION_STRING = os.getenv(\"MONGODB_URI\")\n",
    "MONGODB_DATABASE = os.getenv(\"MONGODB_DATABASE\")\n",
    "MONGODB_COLLECTION = os.getenv(\"MONGODB_COLLECTION\")\n",
    "API_TOKEN = os.getenv(\"API_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text extraction from Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the data from the file\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = fitz.open('/Users/mrinoyb2/git/AyurBot/Data/pdf/Ayurveda_Book.pdf')\n",
    "\n",
    "# Function to preprocess and clean text\n",
    "def preprocess_text_mupdf(text):\n",
    "    # Remove headers/footers\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)  # Remove empty lines\n",
    "    text = re.sub(r'[^A-Za-z0-9.,;:!?()\\'\\\"\\n]+', ' ', text)  # Remove special characters but keep punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
    "    return text.strip()\n",
    "\n",
    "# Extract and clean text\n",
    "cleaned_text_mupdf = \"\"\n",
    "for page_number in range(pdf_document.page_count):\n",
    "    page = pdf_document.load_page(page_number)\n",
    "    text = page.get_text()\n",
    "    cleaned_text_mupdf += preprocess_text_mupdf(text)\n",
    "\n",
    "# Close the PDF document\n",
    "pdf_document.close()\n",
    "\n",
    "# Output the first \n",
    "print(cleaned_text_mupdf)\n",
    "\n",
    "# Save the cleaned text to a file\n",
    "with open('/Users/mrinoyb2/git/AyurBot/Data/clean_text/Ayurveda_Book.txt', 'w') as file:\n",
    "    file.write(cleaned_text_mupdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store chunks in MongoDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# Connect to MongoDB (Update the connection string as per your MongoDB setup)\n",
    "client = pymongo.MongoClient(MONGODB_CONNECTION_STRING)\n",
    "db = client[MONGODB_DATABASE]\n",
    "collection = db[MONGODB_COLLECTION]\n",
    "\n",
    "# Chunking the text by sentence to boost accurate retrieval\n",
    "def chunk_by_sentence(text):\n",
    "    sentences = []\n",
    "    tmp_sentence = \"\"\n",
    "    for char in text:\n",
    "        if char in [\".\", \"!\", \"?\"]:\n",
    "            sentences.append(tmp_sentence)\n",
    "            tmp_sentence = \"\"\n",
    "        else:\n",
    "            tmp_sentence += char\n",
    "    # Add any remaining text as the last sentence\n",
    "    if tmp_sentence:\n",
    "        sentences.append(tmp_sentence)\n",
    "    return sentences\n",
    "    \n",
    "\n",
    "# Chunk the text\n",
    "chunks = chunk_by_sentence(cleaned_text_mupdf)\n",
    "\n",
    "# Store chunks in MongoDB\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    # Create a document for each chunk\n",
    "    document = {\"_id\": idx, \"text\": chunk}\n",
    "    # Insert the document into the collection\n",
    "    collection.insert_one(document)\n",
    "\n",
    "print(f\"Total chunks stored in MongoDB: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pymongo\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(MONGODB_CONNECTION_STRING)\n",
    "db = client[MONGODB_DATABASE]\n",
    "chunks_collection = db[MONGODB_COLLECTION]\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to update documents with embeddings\n",
    "def update_documents_with_embeddings():\n",
    "    for document in chunks_collection.find():\n",
    "        # Generate embedding\n",
    "        embedding = model.encode(document['text'], convert_to_tensor=False)\n",
    "        # Update document with embedding\n",
    "        chunks_collection.update_one({'_id': document['_id']}, {'$set': {'embedding': embedding.tolist()}})\n",
    "\n",
    "# Uncomment the following line to run the embedding update\n",
    "update_documents_with_embeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic search retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 (Score: 0.653): Whatever the cause of your backache, the following natural Ayurvedic home remedies will be helpful....\n",
      "Result 2 (Score: 0.645): Following these guidelines will help you heal your aching back as well as avoid backpain in the futu...\n",
      "Result 3 (Score: 0.563): Backache can also be relieved by the use of the herb musta, which is a muscle painkiller....\n",
      "Result 4 (Score: 0.529): Some gentle yoga exercises can help with back pain....\n",
      "Result 5 (Score: 0.519): For extra healing and muscle relaxation, apply the mahanarayan oil on your back and then follow with...\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pymongo\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(MONGODB_CONNECTION_STRING)\n",
    "db = client[MONGODB_DATABASE]\n",
    "chunks_collection = db[MONGODB_COLLECTION]\n",
    "\n",
    "# Function to perform semantic search\n",
    "def semantic_search(query, top_k=5):\n",
    "    # Generate query embedding\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "    \n",
    "    # Retrieve all embeddings from MongoDB and calculate similarity\n",
    "    similarities = []\n",
    "    for document in chunks_collection.find():\n",
    "        doc_embedding = np.array(document['embedding'])\n",
    "        similarity = 1 - cosine(query_embedding, doc_embedding)  # Higher score means more similar\n",
    "        similarities.append((document['_id'], similarity, document['text']))\n",
    "    \n",
    "    # Sort by similarity score in descending order\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top_k most similar documents\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Example usage\n",
    "query = \"How to cure backpain?\"\n",
    "results = semantic_search(query)\n",
    "for idx, (doc_id, similarity, text) in enumerate(results, start=1):\n",
    "    print(f\"Result {idx} (Score: {similarity:.3f}): {text[:100]}...\")  # Print the first 100 characters for brevity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "from replicate.client import Client\n",
    "import os\n",
    "\n",
    "# Function to generate an answer using RAG enabled LLama2 from Replicate\n",
    "def generate_RAG_answer(question, max_context_length=1000):\n",
    "    # Assume semantic_search is defined and returns relevant context as a single string\n",
    "    context_results = semantic_search(question, top_k=1)\n",
    "    context = context_results[0][2]  # Get the text of the top result\n",
    "    # Truncate context if it exceeds the maximum length\n",
    "    if len(context) > max_context_length:\n",
    "        context = context[:max_context_length]\n",
    "\n",
    "    rag_prompt = f\"[INST]\\nQuestion: {question}\\nContext: {context}\\n[/INST]\"\n",
    "    print(rag_prompt)\n",
    "\n",
    "    client = Client(api_token=API_TOKEN)\n",
    "\n",
    "    # Generate the answer using LLama2 from Replicate\n",
    "    output = client.run(\n",
    "        \"nwhitehead/llama2-7b-chat-gptq:8c1f632f7a9df740bfbe8f6b35e491ddfe5c43a79b43f062f719ccbe03772b52\",\n",
    "        input={\n",
    "            \"seed\": -1,\n",
    "            \"top_k\": 20,\n",
    "            \"top_p\": 1,\n",
    "            \"prompt\": rag_prompt,\n",
    "            \"max_tokens\": 1024,\n",
    "            \"min_tokens\": 1,\n",
    "            \"temperature\": 0.5,\n",
    "            \"repetition_penalty\": 1\n",
    "        }\n",
    "    )\n",
    "    answer = \"\"\n",
    "    for item in output:\n",
    "        answer += item\n",
    "        \n",
    "    # Handle the case where the answer is empty\n",
    "    if answer == \"\":\n",
    "        answer = \"Sorry, I don't have an answer for that.\"\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Function to generate an answer using LLama2 from Replicate\n",
    "def generate_non_RAG_answer(question):\n",
    "    # Assume semantic_search is defined and returns relevant context as a single string\n",
    "    non_rag_prompt = f\"[INST]\\nQuestion: {question}\\n[/INST]\"  # Fallback in case no context is found\n",
    "    print(non_rag_prompt)\n",
    "\n",
    "    client = Client(api_token=API_TOKEN)\n",
    "\n",
    "    # Generate the answer using LLama2 from Replicate\n",
    "    output = client.run(\n",
    "        \"nwhitehead/llama2-7b-chat-gptq:8c1f632f7a9df740bfbe8f6b35e491ddfe5c43a79b43f062f719ccbe03772b52\",\n",
    "        input={\n",
    "            \"seed\": -1,\n",
    "            \"top_k\": 20,\n",
    "            \"top_p\": 1,\n",
    "            \"prompt\": non_rag_prompt,\n",
    "            \"max_tokens\": 1024,\n",
    "            \"min_tokens\": 1,\n",
    "            \"temperature\": 0.5,\n",
    "            \"repetition_penalty\": 1\n",
    "        }\n",
    "    )\n",
    "    answer = \"\"\n",
    "    for item in output:\n",
    "        answer += item\n",
    "        \n",
    "    # Handle the case where the answer is empty\n",
    "    if not answer:\n",
    "        answer = \"Sorry, I don't have an answer for that.\"\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]\n",
      "Question: How to make agni tea?\n",
      "Context: Or try this recipe for a tea to stimulate agni, the digestive fire: Agni Tea 1 quart water pinch cayenne pepper handful minced ginger root 2 tablespoons Sucanat or other sweetener to teaspoon rock salt Put all the above ingredients in a pot and boil for 20 minutes.\n",
      "[/INST]\n",
      "  Great! Here's a step-by-step guide on how to make Agni Tea:\n",
      "Ingredients:\n",
      "* 1 quart water\n",
      "* Pinch of cayenne pepper\n",
      "* Handful of minced ginger root\n",
      "* 2 tablespoons Sucanat or other sweetener (optional)\n",
      "* Rock salt (optional)\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Start by bringing the water to a boil in a pot.\n",
      "2. Once the water is boiling, add the pinch of cayenne pepper and stir well.\n",
      "3. Next, add the handful of minced ginger root to the pot and stir again.\n",
      "4. Add the 2 tablespoons of Sucanat or other sweetener (if using) and stir well.\n",
      "5. If desired, add a small amount of rock salt to the pot and stir to combine.\n",
      "6. Reduce the heat to a simmer and let the tea steep for 20 minutes.\n",
      "7. Strain the tea into a cup and enjoy!\n",
      "Note: You can adjust the amount of cayenne pepper to your liking, depending on how much heat you prefer in your tea. Some people find that a small amount of cayenne pepper has a stimulating effect on digestion, while others may prefer more or less. Experiment and find the amount that works best for you.\n",
      "Agni Tea is a traditional Ayurvedic recipe that is designed to stimulate the digestive fire and promote healthy digestion. The ginger root in the tea helps to warm and stimulate the digestive system, while the cayenne pepper adds a subtle kick of heat that can help to break down food and promote digestion. The sweetener (if used) can help to balance the flavor of the tea and make it more enjoyable to drink.\n",
      "I hope you enjoy making and drinking Agni Tea! Let me know if you have any questions or need further clarification on the recipe.\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"How to make agni tea?\"\n",
    "rag_answer = generate_RAG_answer(query)\n",
    "\n",
    "print(rag_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]\n",
      "Question: How to make agni tea?\n",
      "[/INST]\n",
      "  Agni tea is a traditional Ayurvedic tea that is made by infusing herbs and spices in hot water. Here is a simple recipe for making agni tea at home:\n",
      "Ingredients:\n",
      "\n",
      "* 2 teaspoons of ginger powder\n",
      "* 2 teaspoons of cinnamon powder\n",
      "* 2 teaspoons of black pepper\n",
      "* 1 teaspoon of turmeric powder\n",
      "* 1 teaspoon of ginger oil\n",
      "* 1 teaspoon of cinnamon oil\n",
      "* 1 teaspoon of black pepper oil\n",
      "* 1 teaspoon of honey (optional)\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Boil water in a pot and let it cool to a comfortable temperature for drinking.\n",
      "2. In a small bowl, mix together the ginger powder, cinnamon powder, black pepper, and turmeric powder.\n",
      "3. Add the herbal mixture to the boiling water and stir well.\n",
      "4. Reduce the heat to low and let the tea steep for 5-10 minutes, or until the desired flavor is achieved.\n",
      "5. Strain the tea into a cup and discard the solids.\n",
      "6. Add the ginger oil, cinnamon oil, and black pepper oil to the tea and stir well.\n",
      "7. If desired, add honey to sweeten the tea.\n",
      "8. Serve the agni tea hot, or let it cool to room temperature and drink it as a refreshing beverage.\n",
      "Tips:\n",
      "\n",
      "* You can adjust the amount of each ingredient to suit your taste preferences.\n",
      "* Adding a slice of ginger or a cinnamon stick to the tea can enhance the flavor and aroma.\n",
      "* You can also add other herbs and spices to the tea, such as cloves, cardamom, or fennel seeds, to create a unique flavor profile.\n",
      "* Agni tea can be consumed as a daily tonic to promote digestion and overall well-being, or as a pre-bedtime tea to aid in relaxation and sleep.\n",
      "* Store any leftover agni tea in an airtight container in the refrigerator for up to 3 days.\n",
      "I hope this helps you make a delicious and healthy agni tea at home!\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"How to make agni tea?\"\n",
    "non_rag_answer = generate_non_RAG_answer(query)\n",
    "\n",
    "print(non_rag_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating RAG vs Non-RAG Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to quantify this, I will take the following steps:\n",
    "1. Think of a query which has a well defined answer in the book.\n",
    "2. Find the true answer to a query from the actual source (book pdf). \n",
    "3. Then pass the query through the RAG based LLM and the regular LLM.\n",
    "4. Save both generated answers along with the true answer and find word embeddings for each. \n",
    "5. Finally, compare the word embeddings of the true answer with the RAG vs Non-RAG based LLM word embeddings using cosine similarity. \n",
    "\n",
    "Following these steps will help quantify the performace and accuracy of information in the two answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query from book \"The Complete Book of Ayurvedic Home Remedies\" by Vasant Lad\n",
    "query = \"How to make agni tea?\"\n",
    "\n",
    "# True Answer from the book.\n",
    "true_answer = \"\"\"1 quart water\n",
    "1⁄8 pinch cayenne pepper\n",
    "1⁄2 handful minced ginger root\n",
    "2 tablespoons Sucanat or other sweetener 1⁄8 to 1⁄2 teaspoon rock salt\n",
    "Put all the above ingredients in a pot and boil for 20 minutes.\n",
    "Take the pot o􏰂 the burner, cool for a few minutes, then add the juice of half a lime. Do not boil the lime juice.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing functions to evaluate performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RAG Similarity Score': 0.6470919847488403, 'Non-RAG Similarity Score': 0.5113146305084229}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"\n",
    "    Generate an embedding for a given text.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text.\n",
    "    \n",
    "    Returns:\n",
    "    - The sentence embedding.\n",
    "    \"\"\"\n",
    "    # Load the sentence transformer model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    # Generate the sentence embeddings\n",
    "    embeddings = model.encode(text, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two embeddings.\n",
    "\n",
    "    Args:\n",
    "    - embedding1 (torch.Tensor): The first embedding.\n",
    "    - embedding2 (torch.Tensor): The second embedding.\n",
    "\n",
    "    Returns:\n",
    "    - The cosine similarity score.\n",
    "    \"\"\"\n",
    "    # Calculate the cosine similarity\n",
    "    if len(embedding1.shape) == 1:\n",
    "        embedding1 = embedding1.unsqueeze(0)\n",
    "    if len(embedding2.shape) == 1:\n",
    "        embedding2 = embedding2.unsqueeze(0)\n",
    "    similarity = torch.nn.functional.cosine_similarity(embedding1, embedding2, dim=1)\n",
    "    return similarity.item()\n",
    "\n",
    "\n",
    "def calculate_similarity_scores(true_answer, rag_answer, non_rag_answer):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity scores between the true answer and both RAG-based and non-RAG-based answers.\n",
    "\n",
    "    Args:\n",
    "    - true_answer (str): The true answer text.\n",
    "    - rag_answer (str): The RAG-based model's answer text.\n",
    "    - non_rag_answer (str): The non-RAG-based model's answer text.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with cosine similarity scores.\n",
    "    \"\"\"\n",
    "    # Convert the answers to embeddings\n",
    "    true_answer_embedding = get_embedding(true_answer)\n",
    "    rag_answer_embedding = get_embedding(rag_answer)\n",
    "    non_rag_answer_embedding = get_embedding(non_rag_answer)\n",
    "    \n",
    "    # Calculate cosine similarity scores\n",
    "    rag_similarity = calculate_cosine_similarity(true_answer_embedding, rag_answer_embedding)\n",
    "    non_rag_similarity = calculate_cosine_similarity(true_answer_embedding, non_rag_answer_embedding)\n",
    "\n",
    "    \n",
    "    # Return the scores\n",
    "    return {\n",
    "        \"RAG Similarity Score\": rag_similarity,\n",
    "        \"Non-RAG Similarity Score\": non_rag_similarity\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate the similarity scores\n",
    "similarity_scores = calculate_similarity_scores(true_answer, rag_answer, non_rag_answer)\n",
    "print(similarity_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
